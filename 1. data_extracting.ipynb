{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21e2fda",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "This notebook facilitates the processing of medical datasets (CSV files). \n",
    "The workflow includes:\n",
    "1. **Loading** data from the `datasets` folders.\n",
    "2. **Sampling** 200 documents from each class.\n",
    "3. **Preprocessing** abstracts into 100-word chunks.\n",
    "4. **Labeling** and consolidating the data into a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f71e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets folder: c:\\Users\\Administrator\\Desktop\\Classification\\datasets\n",
      "Output file: processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Configuration / Settings\n",
    "datasets_folder = 'datasets'\n",
    "output_file = 'processed_data.csv'\n",
    "chunk_size = 100\n",
    "sample_size = 200\n",
    "\n",
    "print(f\"Datasets folder: {os.path.abspath(datasets_folder)}\")\n",
    "print(f\"Output file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c06410",
   "metadata": {},
   "source": [
    "## 1. Process Data Files\n",
    "\n",
    "We iterate through each `.csv` file in the target folder. \n",
    "For each file:\n",
    "- Check for required columns (`Title`, `Abstract`).\n",
    "- Randomly sample documents if the count exceeds the limit.\n",
    "- Split the `Abstract` text into fixed-size word chunks.\n",
    "- Discard key chunks that are too short (less than 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d125bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 CSV files in 'datasets':\n",
      "\n",
      "--- Processing: Alzheimer's Disease.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: Frontotemporal Dementia.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: Lewy Body Dementia.csv ---\n",
      "  Sampling: Selected 200 out of 14268 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: Parkinson's Disease.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "--- Processing: Vascular Dementia.csv ---\n",
      "  Sampling: Selected 200 out of 20000 documents.\n",
      "  > Generated 200 valid records from this file.\n",
      "\n",
      "Processing complete. Total records collected: 1000\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to store results\n",
    "results = []\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(datasets_folder):\n",
    "    print(f\"Error: Folder '{datasets_folder}' not found.\")\n",
    "else:\n",
    "    # Get list of CSV files\n",
    "    files = [f for f in os.listdir(datasets_folder) if f.endswith('.csv')]\n",
    "    print(f\"Found {len(files)} CSV files in '{datasets_folder}':\\n\")\n",
    "\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(datasets_folder, filename)\n",
    "        label = os.path.splitext(filename)[0]\n",
    "        \n",
    "        print(f\"--- Processing: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'Abstract' not in df.columns or 'Title' not in df.columns:\n",
    "                print(f\"  [Skipped] Required columns 'Abstract' or 'Title' missing.\")\n",
    "                continue\n",
    "            \n",
    "            total_docs = len(df)\n",
    "            \n",
    "            # Randomly sample 200 documents\n",
    "            if total_docs > sample_size:\n",
    "                sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "                print(f\"  Sampling: Selected {sample_size} out of {total_docs} documents.\")\n",
    "            else:\n",
    "                sampled_df = df\n",
    "                print(f\"  Taking all: Used all {total_docs} documents.\")\n",
    "            \n",
    "            # Process Abstract content\n",
    "            file_record_count = 0\n",
    "            for _, row in sampled_df.iterrows():\n",
    "                abstract = row['Abstract']\n",
    "                paper_name = row['Title']\n",
    "                \n",
    "                # Fetch additional fields\n",
    "                # Using .get() to avoid errors if columns are missing in some files\n",
    "                doc_type = row.get('Document Type', '')\n",
    "                affiliations = row.get('Affiliations', '')\n",
    "\n",
    "                # Handle missing or non-string abstracts\n",
    "                if pd.isna(abstract) or not isinstance(abstract, str):\n",
    "                    continue\n",
    "                \n",
    "                # Split into words\n",
    "                words = abstract.split()\n",
    "                \n",
    "                # User Requirement: \n",
    "                # 1. Take first 100 words.\n",
    "                # 2. Truncate if longer.\n",
    "                # 3. Pad if shorter.\n",
    "                \n",
    "                if len(words) >= chunk_size:\n",
    "                    chunk = words[:chunk_size]\n",
    "                else:\n",
    "                    # Pad with a placeholder token\n",
    "                    chunk = words + ['[PAD]'] * (chunk_size - len(words))\n",
    "                \n",
    "                chunk_text = ' '.join(chunk)\n",
    "                \n",
    "                results.append({\n",
    "                    'Content': chunk_text,\n",
    "                    'Paper Name': paper_name,\n",
    "                    'Label': label,\n",
    "                    'Document Type': doc_type,\n",
    "                    'Affiliations': affiliations\n",
    "                })\n",
    "                file_record_count += 1\n",
    "            \n",
    "            print(f\"  > Generated {file_record_count} valid records from this file.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Error] Failed to read {filename}: {e}\\n\")\n",
    "\n",
    "print(f\"Processing complete. Total records collected: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9586e56",
   "metadata": {},
   "source": [
    "## 2. Save Results\n",
    "\n",
    "We inspect the resulting dataset (View Shape, Class Distribution, and Head) and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48345d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 1000\n",
      "Total Columns: 5\n",
      "\n",
      "--- Class Distribution (Records per Label) ---\n",
      "Label\n",
      "Alzheimer's Disease        200\n",
      "Frontotemporal Dementia    200\n",
      "Lewy Body Dementia         200\n",
      "Parkinson's Disease        200\n",
      "Vascular Dementia          200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- First 5 Records ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Paper Name</th>\n",
       "      <th>Label</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Affiliations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insulin resistance is a condition characterize...</td>\n",
       "      <td>Brain insulin resistance mediated cognitive im...</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Review</td>\n",
       "      <td>Department of Pharmaceutical Sciences, Maharsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prolactin is a pituitary anterior lobe hormone...</td>\n",
       "      <td>Hyperprolactinemia and Brain Health: Exploring...</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Review</td>\n",
       "      <td>School of Pharmacy, Hubei University of Chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lecanemab is an amyloid-targeted antibody indi...</td>\n",
       "      <td>Severe Persistent Urinary Retention Following ...</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Article</td>\n",
       "      <td>Department of Psychiatry, Duke University Scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Glycoprotein 88 (GP88) is a secreted biomarker...</td>\n",
       "      <td>An Impedimetric Immunosensor for Progranulin D...</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Article</td>\n",
       "      <td>University of New Brunswick, Fredericton, NB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disruption of the blood–brain barrier (BBB) ac...</td>\n",
       "      <td>Regulation of Blood–Brain Barrier Permeability...</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Review</td>\n",
       "      <td>Department of Pharmacology, Research Institute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  \\\n",
       "0  Insulin resistance is a condition characterize...   \n",
       "1  Prolactin is a pituitary anterior lobe hormone...   \n",
       "2  Lecanemab is an amyloid-targeted antibody indi...   \n",
       "3  Glycoprotein 88 (GP88) is a secreted biomarker...   \n",
       "4  Disruption of the blood–brain barrier (BBB) ac...   \n",
       "\n",
       "                                          Paper Name                Label  \\\n",
       "0  Brain insulin resistance mediated cognitive im...  Alzheimer's Disease   \n",
       "1  Hyperprolactinemia and Brain Health: Exploring...  Alzheimer's Disease   \n",
       "2  Severe Persistent Urinary Retention Following ...  Alzheimer's Disease   \n",
       "3  An Impedimetric Immunosensor for Progranulin D...  Alzheimer's Disease   \n",
       "4  Regulation of Blood–Brain Barrier Permeability...  Alzheimer's Disease   \n",
       "\n",
       "  Document Type                                       Affiliations  \n",
       "0        Review  Department of Pharmaceutical Sciences, Maharsh...  \n",
       "1        Review  School of Pharmacy, Hubei University of Chines...  \n",
       "2       Article  Department of Psychiatry, Duke University Scho...  \n",
       "3       Article  University of New Brunswick, Fredericton, NB, ...  \n",
       "4        Review  Department of Pharmacology, Research Institute...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Success] Data saved to: processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # --- Display Info ---\n",
    "    print(f\"Total Rows: {result_df.shape[0]}\")\n",
    "    print(f\"Total Columns: {result_df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n--- Class Distribution (Records per Label) ---\")\n",
    "    print(result_df['Label'].value_counts())\n",
    "    \n",
    "    print(\"\\n--- First 5 Records ---\")\n",
    "    \n",
    "    try:\n",
    "        display(result_df.head())\n",
    "    except NameError:\n",
    "        print(result_df.head())\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n[Success] Data saved to: {output_file}\")\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"[Warning] No data generated. Review the source files and logic.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
